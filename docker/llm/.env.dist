# Resource:
# https://github.com/ollama/ollama/blob/main/docs/faq.md

# The address and port at which the ollama application binds and lists for incoming connections.
# The default is 127.0.0.1 port 11434.
OLLAMA_HOST=0.0.0.0:11434

# Serves as the default keep alive value for model api calls.
# -1 keeps all models loaded into memory.
# Resource:
# https://github.com/ollama/ollama/blob/c02db93243353855b983db2a1562a02b57e66db1/docs/faq.md?plain=1#L214
# The default value is 5m.
# OLLAMA_KEEP_ALIVE=-1

# The maximum number of models that can be loaded concurrently provided they fit in available memory.
# The default is 3 * the number of GPUs or 3 for CPU inference.
# OLLAMA_MAX_LOADED_MODELS=3

# The maximum number of parallel requests each model will process at the same time.
# The default will auto-select either 4 or 1 based on available memory.
# OLLAMA_NUM_PARALLEL=4

# The maximum number of requests Ollama will queue when busy before rejecting additional requests.
# The default is 512
# OLLAMA_MAX_QUEUE=512

# Flash Attention is a feature of most modern models that can significantly reduce memory usage as the context size grows.
# To enable Flash Attention, set the OLLAMA_FLASH_ATTENTION environment variable to 1 when starting the Ollama server.
# OLLAMA_FLASH_ATTENTION=0

# The K/V context cache can be quantized to significantly reduce memory usage when Flash Attention is enabled.
# The currently available K/V cache quantization types are:
# + f16 - high precision and memory usage (default).
# + q8_0 - 8-bit quantization, uses approximately 1/2 the memory of f16 with a very small loss in precision, this usually has no noticeable impact on the model's quality (recommended if not using f16).
# + q4_0 - 4-bit quantization, uses approximately 1/4 the memory of f16 with a small-medium loss in precision that may be more noticeable at higher context sizes.
# The default is f16
# OLLAMA_KV_CACHE_TYPE=f16
