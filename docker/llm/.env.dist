# Resource:
# https://github.com/ollama/ollama/blob/main/docs/faq.md

# CUDA_VISIBLE_DEVICES:
# GPU_DEVICE_ORDINAL:
# HIP_VISIBLE_DEVICES:
# HSA_OVERRIDE_GFX_VERSION:
# HTTPS_PROXY:
# HTTP_PROXY:
# NO_PROXY:
# OLLAMA_CONTEXT_LENGTH:2048
# OLLAMA_DEBUG:false

# Flash Attention is a feature of most modern models that can significantly reduce memory usage as the context size grows.
# To enable Flash Attention, set the OLLAMA_FLASH_ATTENTION environment variable to 1 when starting the Ollama server.
# OLLAMA_FLASH_ATTENTION=false

# OLLAMA_GPU_OVERHEAD=0

# The address and port at which the ollama application binds and lists for incoming connections.
# The default is 127.0.0.1 port 11434.
OLLAMA_HOST=0.0.0.0:11434

# OLLAMA_INTEL_GPU:false

# Serves as the default keep alive value for model api calls.
# -1 keeps all models loaded into memory.
# Resource:
# https://github.com/ollama/ollama/blob/c02db93243353855b983db2a1562a02b57e66db1/docs/faq.md?plain=1#L214
# The default value is 5m0s.
# OLLAMA_KEEP_ALIVE=-1

# The K/V context cache can be quantized to significantly reduce memory usage when Flash Attention is enabled.
# The currently available K/V cache quantization types are:
# + f16 - high precision and memory usage (default).
# + q8_0 - 8-bit quantization, uses approximately 1/2 the memory of f16 with a very small loss in precision, this usually has no noticeable impact on the model's quality (recommended if not using f16).
# + q4_0 - 4-bit quantization, uses approximately 1/4 the memory of f16 with a small-medium loss in precision that may be more noticeable at higher context sizes.
# The default is f16
# OLLAMA_KV_CACHE_TYPE=f16

# OLLAMA_LLM_LIBRARY:
# OLLAMA_LOAD_TIMEOUT:5m0s

# The maximum number of models that can be loaded concurrently provided they fit in available memory.
# The default is 3 * the number of GPUs or 3 for CPU inference.
OLLAMA_MAX_LOADED_MODELS=2

# The maximum number of requests Ollama will queue when busy before rejecting additional requests.
# The default is 512
# OLLAMA_MAX_QUEUE=512

# OLLAMA_MODELS:/root/.ollama/models
# OLLAMA_MULTIUSER_CACHE:false
# OLLAMA_NEW_ENGINE:false
# OLLAMA_NOHISTORY:false
# OLLAMA_NOPRUNE:false

# The maximum number of parallel requests each model will process at the same time.
# The default will auto-select either 4 or 1 based on available memory.
# OLLAMA_NUM_PARALLEL=2

# OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*]
# OLLAMA_SCHED_SPREAD:false
# ROCR_VISIBLE_DEVICES:
# http_proxy:
# https_proxy:
# no_proxy:
